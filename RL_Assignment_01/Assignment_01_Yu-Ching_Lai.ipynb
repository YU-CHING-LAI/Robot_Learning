{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1) \n",
    "\n",
    "Given is a six-armed bandit, as introduced in the lecture.\n",
    "\n",
    "The first arm shall sample its reward uniformly from the interval [-1, 4).\n",
    "\n",
    "The second arm shall sample its reward uniformly from [2, 6).\n",
    "\n",
    "The third arm shall sample its reward uniformly from the interval [-2, 3).\n",
    "\n",
    "The fourth arm shall sample its reward uniformly from [5, 9).\n",
    "\n",
    "The fifth arm shall sample its reward uniformly from [-3, 5).\n",
    "\n",
    "The sixth arm shall sample its reward uniformly from [1, 6).\n",
    "\n",
    "What is the expected reward when actions are chosen uniformly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1.1 Solution:\n",
    "\n",
    "Because every arms sample their reward uniformly,\n",
    "\n",
    "the expected value for the first arm is $\\frac{-1+4}{2}=1.5$,\n",
    "\n",
    "the expected value for the second arm is $\\frac{2+6}{2}=4$,\n",
    "\n",
    "the expected value for the third arm is $\\frac{-2+3}{2}=0.5$,\n",
    "\n",
    "the expected value for the forth arm is $\\frac{5+9}{2}=7$,\n",
    "\n",
    "the expected value for the fifth arm is $\\frac{-3+5}{2}=1$,\n",
    "\n",
    "the expected value for the sixth arm is $\\frac{1+6}{2}=3.5$.\n",
    "\n",
    "And because the action are also chosen uniformly, the expected reward is $\\frac{1.5+4+0.5+7+1+3.5}{6}= \\frac{1.75}{6} \\approx  2.9167$. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2)\n",
    "\n",
    "Implement the six-armed bandit from 1.1) and compute the sample average reward for 20 uniformly chosen actions!\n",
    "\n",
    "Compare this to your expectation from 1.1)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.941007175814138\n"
     ]
    }
   ],
   "source": [
    "# 1.2 Solution:\n",
    "import random\n",
    "\n",
    "arms = [\n",
    "    lambda: random.uniform(-1,4),\n",
    "    lambda: random.uniform(2,6),\n",
    "    lambda: random.uniform(-2,3),\n",
    "    lambda: random.uniform(5,9),\n",
    "    lambda: random.uniform(-3,5),\n",
    "    lambda: random.uniform(1,6)\n",
    "]\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for _ in range(20):\n",
    "    chose = random.randint(0,5)\n",
    "    reward = arms[chose]()\n",
    "    rewards.append(reward)\n",
    "    \n",
    "reward_average = sum(rewards) / len(rewards)\n",
    "\n",
    "print(reward_average)\n",
    "\n",
    "# The result averages will change with every attempt, but they are usually close to the expectation from 1.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3) \n",
    "\n",
    "Initialize Q(ai)=0 and chose 2000 actions according to an ε-greedy selection strategy (ε=0.1)!\n",
    "\n",
    "Update your action values by computing the sample average reward of each action recursively!\n",
    "\n",
    "For every 100 actions show the percentage of choosing arm 1, arm 2, arm 3, arm 4, arm 5, and\n",
    "arm 6 as well as the resulting average reward!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100:\n",
      "percentage: ['0.0%', '1.0%', '2.0%', '78.0%', '19.0%', '0.0%']\n",
      "average reward: 5.70\n",
      "Step 200:\n",
      "percentage: ['0.5%', '2.0%', '1.0%', '86.0%', '10.0%', '0.5%']\n",
      "average reward: 6.04\n",
      "Step 300:\n",
      "percentage: ['1.0%', '2.0%', '1.3%', '88.0%', '7.3%', '0.3%']\n",
      "average reward: 6.17\n",
      "Step 400:\n",
      "percentage: ['1.2%', '2.8%', '1.8%', '87.8%', '5.8%', '0.8%']\n",
      "average reward: 6.22\n",
      "Step 500:\n",
      "percentage: ['1.4%', '2.2%', '1.8%', '88.4%', '5.2%', '1.0%']\n",
      "average reward: 6.28\n",
      "Step 600:\n",
      "percentage: ['1.2%', '2.0%', '1.7%', '89.3%', '4.7%', '1.2%']\n",
      "average reward: 6.33\n",
      "Step 700:\n",
      "percentage: ['1.1%', '1.7%', '1.6%', '90.1%', '4.1%', '1.3%']\n",
      "average reward: 6.38\n",
      "Step 800:\n",
      "percentage: ['1.5%', '1.8%', '1.4%', '90.4%', '3.6%', '1.4%']\n",
      "average reward: 6.43\n",
      "Step 900:\n",
      "percentage: ['1.6%', '1.9%', '1.4%', '90.4%', '3.2%', '1.4%']\n",
      "average reward: 6.45\n",
      "Step 1000:\n",
      "percentage: ['1.4%', '1.8%', '1.4%', '90.9%', '3.1%', '1.4%']\n",
      "average reward: 6.48\n",
      "Step 1100:\n",
      "percentage: ['1.7%', '1.6%', '1.3%', '91.0%', '2.9%', '1.5%']\n",
      "average reward: 6.50\n",
      "Step 1200:\n",
      "percentage: ['1.7%', '1.7%', '1.3%', '91.1%', '2.7%', '1.6%']\n",
      "average reward: 6.50\n",
      "Step 1300:\n",
      "percentage: ['1.8%', '1.6%', '1.2%', '91.2%', '2.5%', '1.7%']\n",
      "average reward: 6.52\n",
      "Step 1400:\n",
      "percentage: ['1.8%', '1.6%', '1.1%', '91.3%', '2.4%', '1.8%']\n",
      "average reward: 6.53\n",
      "Step 1500:\n",
      "percentage: ['1.8%', '1.7%', '1.2%', '91.3%', '2.3%', '1.7%']\n",
      "average reward: 6.54\n",
      "Step 1600:\n",
      "percentage: ['1.9%', '1.6%', '1.2%', '91.5%', '2.3%', '1.6%']\n",
      "average reward: 6.55\n",
      "Step 1700:\n",
      "percentage: ['1.8%', '1.5%', '1.3%', '91.6%', '2.2%', '1.6%']\n",
      "average reward: 6.56\n",
      "Step 1800:\n",
      "percentage: ['1.8%', '1.4%', '1.3%', '91.6%', '2.1%', '1.7%']\n",
      "average reward: 6.56\n",
      "Step 1900:\n",
      "percentage: ['1.8%', '1.5%', '1.4%', '91.5%', '2.2%', '1.6%']\n",
      "average reward: 6.54\n",
      "Step 2000:\n",
      "percentage: ['1.8%', '1.4%', '1.5%', '91.6%', '2.1%', '1.6%']\n",
      "average reward: 6.55\n"
     ]
    }
   ],
   "source": [
    "Q = [0.0 for _ in range(6)] # list have 6 initial values 0.0\n",
    "N = [0.0 for _ in range(6)]\n",
    "\n",
    "epsilon = 0.1\n",
    "total_reward = 0\n",
    "reward_record = []\n",
    "choice_record = []\n",
    "\n",
    "for t in range(1, 2001):\n",
    "    if random.random() < epsilon: # generate a number between [0,1)\n",
    "        action = random.randint(0,5) #exploit\n",
    "    else:\n",
    "        max_Q = max(Q)\n",
    "        best_actions = [i for i, q in enumerate(Q) if q == max_Q] # index of biggest Q\n",
    "        action = random.choice(best_actions) # choice one arm\n",
    "    reward = arms[action]()\n",
    "    N[action] += 1 # times this arm has been chosen \n",
    "    Q[action] +=(reward - Q[action]) / N[action] # update\n",
    "    total_reward += reward\n",
    "    \n",
    "    if t % 100 == 0:\n",
    "        percentage = [n/t*100 for n in N]\n",
    "        avg_reward = total_reward / t\n",
    "        choice_record.append(percentage)\n",
    "        reward_record.append(avg_reward)\n",
    "        \n",
    "for i in range(20):\n",
    "    print(f\"Step {(i+1)*100}:\")\n",
    "    print(f\"percentage: {[f'{p:.1f}%' for p in choice_record[i]]}\")\n",
    "    print(f\"average reward: {reward_record[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4) \n",
    "\n",
    "Redo the experiment, but after 1000 steps sample the rewards of the fourth arm uniformly from [-4, 3) !\n",
    "\n",
    "Compare updating action values by computing the sample average reward of each action recursively (as done in 1.3) with using a constant learning rate α=0.05 !\n",
    "\n",
    "For every 100 actions show the percentage of choosing arm 1, arm 2, arm 3, arm 4, arm 5, and arm 6 as well as the resulting average reward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100\n",
      "Reward: 3.8727222076784344\n",
      "-\n",
      "Step: 200\n",
      "Reward: 3.7577008492213237\n",
      "-\n",
      "Step: 300\n",
      "Reward: 3.7624947060251936\n",
      "-\n",
      "Step: 400\n",
      "Reward: 3.7624773417561697\n",
      "-\n",
      "Step: 500\n",
      "Reward: 3.7934880077963777\n",
      "-\n",
      "Step: 600\n",
      "Reward: 3.8542423018636387\n",
      "-\n",
      "Step: 700\n",
      "Reward: 3.8548903925932207\n",
      "-\n",
      "Step: 800\n",
      "Reward: 4.219193281271563\n",
      "-\n",
      "Step: 900\n",
      "Reward: 4.485223212868166\n",
      "-\n",
      "Step: 1000\n",
      "Reward: 4.68479642333006\n",
      "-\n",
      "Step: 1100\n",
      "Reward: 4.571684767218715\n",
      "-\n",
      "Step: 1200\n",
      "Reward: 4.489772264806794\n",
      "-\n",
      "Step: 1300\n",
      "Reward: 4.447858854827328\n",
      "-\n",
      "Step: 1400\n",
      "Reward: 4.39841138740666\n",
      "-\n",
      "Step: 1500\n",
      "Reward: 4.362820333048134\n",
      "-\n",
      "Step: 1600\n",
      "Reward: 4.330243414628928\n",
      "-\n",
      "Step: 1700\n",
      "Reward: 4.301059546813941\n",
      "-\n",
      "Step: 1800\n",
      "Reward: 4.269885996423775\n",
      "-\n",
      "Step: 1900\n",
      "Reward: 4.248817035407432\n",
      "-\n",
      "Step: 2000\n",
      "Reward: 4.222946084337146\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def arm1(): return random.uniform(-1, 4)\n",
    "def arm2(): return random.uniform(2, 6)\n",
    "def arm3(): return random.uniform(-2, 3)\n",
    "def arm4_before(): return random.uniform(5, 9)\n",
    "def arm4_after(): return random.uniform(-4, 3) # changed\n",
    "def arm5(): return random.uniform(-3, 5)\n",
    "def arm6(): return random.uniform(1, 6)\n",
    "\n",
    "arms = [arm1, arm2, arm3, arm4_before, arm5, arm6]\n",
    "epsilon = 0.1\n",
    "alpha = 0.05\n",
    "steps = 2000\n",
    "\n",
    "Q_const = [0.0 for _ in range(6)]\n",
    "N_avg = [0 for _ in range(6)]\n",
    "\n",
    "counts_const = [0 for _ in range(6)] # times chosen\n",
    "total_reward_avg = 0\n",
    "total_reward_const = 0\n",
    "\n",
    "for step in range(1, steps + 1):\n",
    "    if step == 1001: # after 1000 steps\n",
    "        arms[3] = arm4_after # change\n",
    "    if random.random() < epsilon:\n",
    "        action_const = random.randint(0, 5)\n",
    "    else:\n",
    "        max_q = max(Q_const)\n",
    "        best_actions = [i for i, q in enumerate(Q_const) if q == max_q]\n",
    "        action_const = random.choice(best_actions)\n",
    "\n",
    "    reward_const = arms[action_const]()\n",
    "    Q_const[action_const] += alpha * (reward_const - Q_const[action_const]) # for non-stationary problem\n",
    "    total_reward_const += reward_const\n",
    "    counts_const[action_const] += 1\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        const_r1 = total_reward_const / step\n",
    "        print(f\"Step: {step}\")\n",
    "        print(f\"Reward: {const_r1}\")\n",
    "        print(\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5)\n",
    "\n",
    "Modify the experiment from 1.4) by using an optimistic initialization Q(ai)=10 and a greedy action selection strategy, still using a constant learning rate α=0.05 !\n",
    "\n",
    "For every 100 actions show the percentage of choosing arm 1, arm 2, arm 3, arm 4, arm 5, and arm 6 as well as the resulting average reward !\n",
    "\n",
    "Compare this to your result from 1.4)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100\n",
      "Reward: 5.082779350799684\n",
      "-\n",
      "Step: 200\n",
      "Reward: 5.696743324625934\n",
      "-\n",
      "Step: 300\n",
      "Reward: 6.156532020953602\n",
      "-\n",
      "Step: 400\n",
      "Reward: 6.366728506347495\n",
      "-\n",
      "Step: 500\n",
      "Reward: 6.463887075826089\n",
      "-\n",
      "Step: 600\n",
      "Reward: 6.56843172721112\n",
      "-\n",
      "Step: 700\n",
      "Reward: 6.6302129700052514\n",
      "-\n",
      "Step: 800\n",
      "Reward: 6.67435233627016\n",
      "-\n",
      "Step: 900\n",
      "Reward: 6.6693282248418155\n",
      "-\n",
      "Step: 1000\n",
      "Reward: 6.732445969994705\n",
      "-\n",
      "Step: 1100\n",
      "Reward: 6.365089463840575\n",
      "-\n",
      "Step: 1200\n",
      "Reward: 6.104655321261859\n",
      "-\n",
      "Step: 1300\n",
      "Reward: 5.948215085706159\n",
      "-\n",
      "Step: 1400\n",
      "Reward: 5.8203552493472746\n",
      "-\n",
      "Step: 1500\n",
      "Reward: 5.700699748449232\n",
      "-\n",
      "Step: 1600\n",
      "Reward: 5.598926532514327\n",
      "-\n",
      "Step: 1700\n",
      "Reward: 5.49387561496224\n",
      "-\n",
      "Step: 1800\n",
      "Reward: 5.384944376846615\n",
      "-\n",
      "Step: 1900\n",
      "Reward: 5.30911030050158\n",
      "-\n",
      "Step: 2000\n",
      "Reward: 5.254098611302669\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "arms = [arm1, arm2, arm3, arm4_before, arm5, arm6]\n",
    "steps = 2000\n",
    "alpha = 0.05\n",
    "epsilon = 0 # greedy\n",
    "Q = [10.0 for _ in range(6)]  # optimistic initialization\n",
    "counts = [0 for _ in range(6)]\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(1, steps + 1):\n",
    "    if step == 1001:\n",
    "        arms[3] = arm4_after\n",
    "    # greedy\n",
    "    max_q = max(Q)\n",
    "    best_actions = [i for i, q in enumerate(Q) if q == max_q]\n",
    "    action = random.choice(best_actions)\n",
    "    \n",
    "    reward = arms[action]()\n",
    "    Q[action] += alpha * (reward - Q[action])  \n",
    "    total_reward += reward\n",
    "    counts[action] += 1\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        avg_reward = total_reward / step\n",
    "        print(f\"Step: {step}\")\n",
    "        print(f\"Reward: {avg_reward}\")\n",
    "        print(\"-\")\n",
    "        \n",
    "# Optimistic initial values (1.5) lead to faster reward gains early on compared to ε-greedy exploration (1.4)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mba_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
